{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import json\n",
    "with open('ml_100k.json', 'r') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequences and labels\n",
    "sequences = [entry[0].split(\" | \") for entry in data]\n",
    "labels = [entry[1] for entry in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for items to indices\n",
    "unique_items = set(item for seq in sequences for item in seq)\n",
    "unique_items.update(labels)\n",
    "item_to_idx = {item: idx + 1 for idx, item in enumerate(unique_items)}  # Start indices from 1\n",
    "idx_to_item = {idx: item for item, idx in item_to_idx.items()}\n",
    "num_items = len(item_to_idx) + 1  # Include padding index (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sequences and labels\n",
    "encoded_sequences = [[item_to_idx[item] for item in seq] for seq in sequences]\n",
    "encoded_labels = [item_to_idx[label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "max_seq_len = 50\n",
    "padded_sequences = pad_sequences(encoded_sequences, maxlen=max_seq_len, padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_size = int(0.8 * len(padded_sequences))\n",
    "X_train, X_val = padded_sequences[:train_size], padded_sequences[train_size:]\n",
    "y_train, y_val = np.array(encoded_labels[:train_size]), np.array(encoded_labels[train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SASRec components\n",
    "class SASRec(Model):\n",
    "    def __init__(self, num_items, embedding_dim=64, num_heads=4, num_layers=2, dropout_rate=0.2):\n",
    "        super(SASRec, self).__init__()\n",
    "        self.item_embedding = Embedding(num_items, embedding_dim, mask_zero=True)\n",
    "        self.position_embedding = Embedding(max_seq_len, embedding_dim)\n",
    "        self.attention_layers = [\n",
    "            tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.layer_norms = [LayerNormalization(epsilon=1e-6) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.dense = Dense(num_items)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        seq, positions = inputs\n",
    "        seq_emb = self.item_embedding(seq) + self.position_embedding(positions)\n",
    "        mask = tf.cast(tf.not_equal(seq, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
    "        \n",
    "        for attn, ln in zip(self.attention_layers, self.layer_norms):\n",
    "            attn_output = attn(seq_emb, seq_emb, attention_mask=mask)\n",
    "            seq_emb = ln(seq_emb + attn_output)\n",
    "            seq_emb = self.dropout(seq_emb, training=training)\n",
    "        \n",
    "        return self.dense(seq_emb[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build positional indices\n",
    "positions = np.tile(np.arange(max_seq_len), (len(padded_sequences), 1))\n",
    "positions = pad_sequences(positions, maxlen=max_seq_len, padding='pre', truncating='pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile the model\n",
    "embedding_dim = 64\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = SASRec(num_items=num_items, embedding_dim=embedding_dim, num_heads=num_heads, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0035 - loss: 7.3290 - val_accuracy: 0.0000e+00 - val_loss: 7.3139\n",
      "Epoch 2/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0092 - loss: 7.3139 - val_accuracy: 0.0000e+00 - val_loss: 7.3139\n",
      "Epoch 3/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0060 - loss: 7.3139 - val_accuracy: 0.0000e+00 - val_loss: 7.3139\n",
      "Epoch 4/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0039 - loss: 7.3139 - val_accuracy: 0.0000e+00 - val_loss: 7.3139\n",
      "Epoch 5/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 8.8899e-04 - loss: 7.3139 - val_accuracy: 0.0000e+00 - val_loss: 7.3139\n",
      "Epoch 6/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0073 - loss: 7.3160 - val_accuracy: 0.0053 - val_loss: 7.3139\n",
      "Epoch 7/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0077 - loss: 7.3234 - val_accuracy: 0.0000e+00 - val_loss: 7.3139\n",
      "Epoch 8/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0037 - loss: 7.3156 - val_accuracy: 0.0000e+00 - val_loss: 7.3139\n",
      "Epoch 9/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.0122 - loss: 7.3290 - val_accuracy: 0.0000e+00 - val_loss: 7.3139\n",
      "Epoch 10/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0019 - loss: 7.3139 - val_accuracy: 0.0053 - val_loss: 7.3139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x209e02ae780>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "model.fit(\n",
    "    x=(X_train, positions[:train_size]),\n",
    "    y=y_train,\n",
    "    validation_data=((X_val, positions[train_size:]), y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('sasrec_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step\n"
     ]
    }
   ],
   "source": [
    "test_sequence = [\"The Rock\", \"Titanic\", \"Jurassic Park\"]\n",
    "encoded_test_sequence = pad_sequences([[item_to_idx[item] for item in test_sequence]], maxlen=max_seq_len, padding='pre')\n",
    "predictions = model.predict((encoded_test_sequence, positions[:1]))\n",
    "top_items = np.argsort(predictions[0])[-10:][::-1]\n",
    "recommended_items = [idx_to_item[idx] for idx in top_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Items: ['Striptease', 'Bound', 'Washington Square', 'Backbeat', 'The Rock', 'Home Alone 3', 'Afterglow', 'The Blue Angel, The (Blaue Engel,', 'The Associate', 'Hear My Song']\n"
     ]
    }
   ],
   "source": [
    "print(\"Recommended Items:\", recommended_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@10: 0.0106\n",
      "NDCG@10: 0.0065\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, sequences, labels, item_to_idx, max_seq_len, k=10):\n",
    "    hits, ndcgs = [], []\n",
    "\n",
    "    positions = np.tile(np.arange(max_seq_len), (len(sequences), 1))\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_len, padding='pre', truncating='pre')\n",
    "\n",
    "    for seq, label in zip(padded_sequences, labels):\n",
    "        seq = np.expand_dims(seq, axis=0)\n",
    "        pos = np.expand_dims(positions[0], axis=0)\n",
    "        \n",
    "        predictions = model.predict((seq, pos), verbose=0).flatten()\n",
    "\n",
    "        top_k_items = np.argsort(predictions)[-k:][::-1]\n",
    "        label_idx = item_to_idx[label]\n",
    "        \n",
    "        if label_idx in top_k_items:\n",
    "            hits.append(1)\n",
    "        else:\n",
    "            hits.append(0)\n",
    "        \n",
    "        if label_idx in top_k_items:\n",
    "            rank = np.where(top_k_items == label_idx)[0][0] + 1  # 1-based rank\n",
    "            ndcgs.append(1 / np.log2(rank + 1))\n",
    "        else:\n",
    "            ndcgs.append(0)\n",
    "    \n",
    "    hr = np.mean(hits)\n",
    "    ndcg = np.mean(ndcgs)\n",
    "    \n",
    "    return hr, ndcg\n",
    "\n",
    "test_sequences = X_val\n",
    "test_labels = [idx_to_item[idx] for idx in y_val]\n",
    "\n",
    "hr, ndcg = evaluate_model(model, test_sequences, test_labels, item_to_idx, max_seq_len, k=19)\n",
    "\n",
    "print(f\"HR@10: {hr:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "HR@10: 0.0106\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_hr_at_k(model, X, positions, y, k=10):\n",
    "    predictions = model.predict((X, positions), batch_size=64)\n",
    "    top_k_indices = np.argsort(predictions, axis=1)[:, -k:][:, ::-1]  \n",
    "    hits = np.any(top_k_indices == y[:, np.newaxis], axis=1)\n",
    "    hr_at_k = np.mean(hits)\n",
    "    \n",
    "    return hr_at_k\n",
    "\n",
    "hr_at_10 = evaluate_hr_at_k(model, X_val, positions[train_size:], y_val, k=19)\n",
    "print(f\"HR@10: {hr_at_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
